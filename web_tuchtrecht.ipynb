{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "khZ8mI-weajL"
   },
   "source": [
    "### Purpose of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, urllib.parse\n",
    "\n",
    "import bs4\n",
    "\n",
    "import wetsuite.datasets\n",
    "from wetsuite.helpers import net, localdata, notebook, etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "detail_pages    = localdata.LocalKV('tuchtrecht_detailpages.db',        key_type=str, value_type=bytes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result pages URLs we have fetched, and have still to fetch, in this particular crawl\n",
    "pagination_pages_to_fetch  = set()  # our TODO list\n",
    "pagination_pages_fetched   = set()  # URLs of pagination pages we have fetched and handled, and shouldn't add to fetch again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## seed with the first page of a search\n",
    "# You can use the browser URL after any search, e.g. Scheepvaart is a small example with only a few dozen items:\n",
    "pagination_pages_to_fetch.add( f'https://tuchtrecht.overheid.nl/zoeken/resultaat?ftsscope=uitspraak&domein=Scheepvaart&datumtype=uitspraak' ) \n",
    "\n",
    "#or, if you just want a _lot_ of text, everything:\n",
    "#pagination_pages_to_fetch.add( f'https://tuchtrecht.overheid.nl/zoeken/resultaat?ftsscope=uitspraak&datumtype=uitspraak' ) \n",
    "\n",
    "# If you're fetching everything, and being nice to the server, expect this to take on the order of days.\n",
    "\n",
    "\n",
    "# (we _could_ do a progress bar, because that first page also mentions the highest-numbered page)\n",
    "while len(pagination_pages_to_fetch) > 0: # we keep adding pages on the way, and will eventually exchaust them\n",
    "\n",
    "    ## pick and fetch another page.\n",
    "    # pagination pages are not cached, the pages will change with each new case\n",
    "    fetching_page_url = pagination_pages_to_fetch.pop()\n",
    "    print( f' ========== PAGE: {fetching_page_url} ============ ')\n",
    "    try:\n",
    "        pagebytes = net.download( fetching_page_url ) \n",
    "        pagination_pages_fetched.add( fetching_page_url )\n",
    "    except ValueError:\n",
    "        print(\"HACKY POSTPONE\", fetching_page_url)\n",
    "    \n",
    "    ## parse the pagination HTML page so we can find the cases and further pagination\n",
    "    soup = bs4.BeautifulSoup(pagebytes, features='lxml')\n",
    "\n",
    "    # extract all links to other pagination pages (part of this crawl)\n",
    "    for page_link_a in soup.select(\"div[class*='pagination'] a[href*='&page=']\"):\n",
    "        href = page_link_a.get('href')\n",
    "        page_abs_href = urllib.parse.urljoin( fetching_page_url, href)\n",
    "        if page_abs_href not in pagination_pages_fetched:\n",
    "            pagination_pages_to_fetch.add( page_abs_href )\n",
    "    \n",
    "    # extract all links to detail pages (part of fetched data)\n",
    "    for detail_link in soup.select(\"div[class*='column'] a[href*='/uitspraak/']\"):\n",
    "        href = detail_link.get('href')\n",
    "        detail_abs_href = urllib.parse.urljoin( fetching_page_url, href)\n",
    "        try:\n",
    "            _, fromcache = localdata.cached_fetch( detail_pages, detail_abs_href )\n",
    "            \n",
    "            if not fromcache:\n",
    "                print( 'FETCHED', detail_abs_href )\n",
    "                time.sleep(5) # be somewhat nice to the server\n",
    "            #else:\n",
    "            #    print( 'CACHED', detail_abs_href )\n",
    "        except ValueError:\n",
    "            print(\"HACKY POSTPONE\", detail_abs_href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'size_bytes': 1499602944,\n",
       " 'size_readable': '1.4GiB',\n",
       " 'num_items': 43475,\n",
       " 'avgsize_bytes': 34493,\n",
       " 'avgsize_readable': '34KiB'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detail_pages.summary(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(htmlbytes):\n",
    "    ' This takes one such downloaded HTML page, and returns (the metadata in the table as a dict, the plaintext as a single string) '\n",
    "    meta={}\n",
    "    plain=[]\n",
    "    soup = bs4.BeautifulSoup(htmlbytes, features='lxml')\n",
    "    for cc in soup.select('div#content div.column'):\n",
    "        if 'column--sidebar' in cc.get('class'):\n",
    "            continue\n",
    "\n",
    "        for ch in cc.children:\n",
    "            if isinstance(ch, bs4.element.NavigableString):\n",
    "                s = str( str(ch) ).strip()\n",
    "                if len(s) > 0:\n",
    "                    #print( 'S %r'%str(ch) )\n",
    "                    plain.append( str(ch) )\n",
    "            else:\n",
    "                if ch.name in ('h1', 'h2', 'h3', 'h4', 'h5', 'h6'):\n",
    "                    #plain.append('H:')\n",
    "                    plain.append( wetsuite.helpers.etree.html_text( str(ch) ) )\n",
    "                    plain.append('\\n\\n')\n",
    "                elif ch.name == 'table'  and  'table__data-overview' in ch.get('class',''):\n",
    "                    for tr in ch.select('tr'):\n",
    "                        #print('TR %r'%str(tr))\n",
    "                        rowcells = list(tr.find_all(['th','td']))\n",
    "                        if len(rowcells) == 2:\n",
    "                            meta[ rowcells[0].text ] = wetsuite.helpers.etree.html_text( str(rowcells[1]) )\n",
    "                    pass\n",
    "                elif ch.name == 'table':\n",
    "                    #plain.append('T:')\n",
    "                    plain.append( wetsuite.helpers.etree.html_text( str(ch) ) )\n",
    "                    plain.append('\\n\\n')\n",
    "                elif ch.name == 'p':\n",
    "                    #plain.append('P:')\n",
    "                    plain.append( wetsuite.helpers.etree.html_text( str(ch) ) )\n",
    "                    plain.append('\\n\\n')\n",
    "                elif ch.name == 'blockquote':\n",
    "                    #plain.append('Q:')\n",
    "                    plain.append( wetsuite.helpers.etree.html_text( str(ch) ) )\n",
    "                    plain.append('\\n\\n')\n",
    "                elif ch.name == 'pre':\n",
    "                    #plain.append('P:')\n",
    "                    plain.append( wetsuite.helpers.etree.html_text( str(ch) ) )\n",
    "                    plain.append('\\n\\n')\n",
    "                elif ch.name == 'div'  and  'align-right' in ch.get('class',''): # probably page links\n",
    "                    pass \n",
    "                elif ch.name in ('div', 'strong', 'em'):\n",
    "                    #plain.append('b:')\n",
    "                    plain.append( wetsuite.helpers.etree.html_text( str(ch) ) )\n",
    "                    plain.append('\\n')\n",
    "                elif ch.name in ('br', 'hr'):\n",
    "                    plain.append('\\n')\n",
    "                elif ch.name in ('ul', 'ol'):\n",
    "                    #plain.append('L:')\n",
    "                    plain.append( wetsuite.helpers.etree.html_text( str(ch) ) )\n",
    "                    plain.append('\\n')\n",
    "                else:\n",
    "                    print( 'N', ch.name, ch.get('class') )\n",
    "    return meta, ''.join(plain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url, htmlbytes in detail_pages.random_sample(3):\n",
    "    print( url )\n",
    "    meta, plain = parse( htmlbytes )\n",
    "    for k,v in meta.items():\n",
    "        print( '%25s   %r'%(k,v) )\n",
    "    print(plain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real parse and store - this might take ten minutes\n",
    "tuchtrecht_struc = wetsuite.helpers.localdata.MsgpackKV('tuchtrecht-struc.db', str, None)\n",
    "tuchtrecht_struc.truncate()\n",
    "tuchtrecht_struc._put_meta('description_short', '''The text and basic metadata shown in the website cases at tuchtrecht.overheid.nl''' )\n",
    "tuchtrecht_struc._put_meta('description',       '''The text and basic metadata shown in the website cases at tuchtrecht.overheid.nl.\n",
    "\n",
    "A case would look something like:                                    \n",
    "{'meta': {'Beslissingen:': '',\n",
    "          'Datum publicatie:': '20-09-2013',\n",
    "          'Datum uitspraak:': '24-03-1994',\n",
    "          'ECLI:': 'ECLI:NL:TDIVBC:1994:1',\n",
    "          'Inhoudsindicatie:': 'Behandeling hond',\n",
    "          'Onderwerp:': 'Honden',\n",
    "          'Zaaknummer(s):': 'VB 1993-02'},\n",
    " 'plaintext': \"ECLI:NL:TDIVBC:1994:1 Veterinair Beroepscollege 's-Gravenhage \"\n",
    "              '(text omitted for brevity)\\n'\n",
    "              '\\n',\n",
    " 'url': 'https://tuchtrecht.overheid.nl/zoeken/resultaat/uitspraak/1994/ECLI_NL_TDIVBC_1994_1'}\n",
    "\n",
    "'''+wetsuite.datasets.generated_today_text())\n",
    "\n",
    "for url, htmlbytes in notebook.ProgressBar( detail_pages.items() ):\n",
    "    meta, text = parse( htmlbytes )\n",
    "    item = {}\n",
    "    item['url']       = url\n",
    "    item['meta']      = meta\n",
    "    item['plaintext'] = text\n",
    "    tuchtrecht_struc.put( url, item )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'size_bytes': 601862144,\n",
       " 'size_readable': '574MiB',\n",
       " 'num_items': 43475,\n",
       " 'avgsize_bytes': 13844,\n",
       " 'avgsize_readable': '13.5KiB'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuchtrecht_struc.summary(True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
