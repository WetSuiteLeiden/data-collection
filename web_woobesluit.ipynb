{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/WetSuiteLeiden/data-collection/blob/master/web_woobesluit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of this notebook\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch responses to [Woo](https://nl.wikipedia.org/wiki/Wet_openbaarheid_van_bestuur#Wet_open_overheid) requests, that is,\n",
    "the subset available at  [rijksoverheid.nl/documenten](https://www.rijksoverheid.nl/documenten?type=Woo-besluit) if you filter for `type=Woo-besluit`.\n",
    "\n",
    "Which, it should be very clearly pointed out, **is far from all of them**. \n",
    "\n",
    "As far as I know -- and this needs checking -- the way these are currently handles means there just _is_ no central source.\n",
    "- ...that the government provides. The third-party [woogle](https://woogle.wooverheid.nl/search?q=*&page=1&type=2i) is actually your best bet for... well, at least _more_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That website, at a glance:\n",
    "* Browse result pages look like https://www.rijksoverheid.nl/documenten?type=Woo%2Dbesluit&pagina=45\n",
    "\n",
    "* A case's detail page looks like: https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/05/22/besluit-op-woo-verzoek-over-alle-eendenhouderijen-nederland\n",
    "\n",
    "* A response document looks something like https://open.overheid.nl/documenten/76f1a787-3f8a-452c-bf58-5911e1a89bcd/file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is variation both in how detail pages work, and how the fetched documents are structured.\n",
    "\n",
    "Consider:\n",
    "* https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/06/20/besluit-op-woo-verzoek-over-cites-2-b-soorten\n",
    "  - is an overall rejection so only has a decision document\n",
    "\n",
    "* https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/06/22/besluit-op-woo-verzoek-over-de-vergaderingen-van-de-kerngroep-bloembollen\n",
    "  - has a single document that is decision + inventory + contents\n",
    "\n",
    "* https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/06/14/besluit---woo-verzoek-ongebruikelijke-transacties-estland-letland-en-litouwen\n",
    "  - has a separate decision document, and document that is the contents of the response, here a bunch of tables\n",
    "\n",
    "* https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/06/15/besluit-op-woo-verzoek-over-correspondentie-naar-aanleiding-van-fouten-in-de-lijst-met-top-100-ammoniakuitstoters\n",
    "  - has a separate decision, and the bijlage/documents is actually a link to _another_ detail page first (which breaks our mostly-correct assumption that every page is a case)\n",
    "\n",
    "* https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/06/21/besluit-op-woo-verzoek-over-vertraging-van-de-bouw-van-stikstofinstallatie-zuidbroek-ii\n",
    "  - is a separate decision. It also separates inventaris and seven separate content documents, also each via their own detail page.\n",
    "\n",
    "* https://open.overheid.nl/documenten/a645fe8c-6c58-4e75-b355-aa3a97023eb8/file\n",
    "  - has a document that is the decision + inventory, and points out the real data is large files should be requested for via mail\n",
    "\n",
    "* https://open.overheid.nl/documenten/107e45ab-6533-4bda-bfc2-816ce107906e/file\n",
    "  - is images of a besluit document. The PDF contains no text layer / OCR.\n",
    "\n",
    "* https://open.overheid.nl/documenten/ronl-439dfebe8cffecb9a385633cb757ced59de469ee/pdf\n",
    "  - has one page of OCR-less image-of-text, then goes on to actual text in the middle of a sensentece\n",
    "\n",
    "\n",
    "As such, creating a dataset with more consistency than that will take some creativity.\n",
    "\n",
    "Let's only care about the decision motivation for now, and not the attached document(s), so that most of the above considerations can be ignored for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime, random, warnings\n",
    "import urllib.parse, re, pprint\n",
    "\n",
    "import bs4\n",
    "import dateutil.parser\n",
    "\n",
    "import wetsuite.helpers.net\n",
    "import wetsuite.helpers.strings\n",
    "import wetsuite.helpers.notebook\n",
    "import wetsuite.helpers.localdata\n",
    "import wetsuite.helpers.patterns\n",
    "import wetsuite.extras.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch search/browse pagination pages\n",
    "\n",
    "This section fetches only the search summaries and links, not yet the documents.\n",
    "\n",
    "None of this part is cached, as we assume new entries are added regularly,\n",
    "so each fetches an up-to-date version of the search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pages_to_fetch = set()   # set of urls\n",
    "# since we can pick up the other-page links, we can add the first page and it'll pick up the rest of the results. \n",
    "# It turns out browsing (search without parameters) like \n",
    "#result_pages_to_fetch.add('https://www.rijksoverheid.nl/documenten?type=Woo-besluit&pagina=1')\n",
    "# ...will never show more than 50 pages, *10 = 500 cases,\n",
    "#   so we have to ensure we have few enough results in each search. \n",
    "#   The easiest way seems to do that in date increments.\n",
    "\n",
    "result_pages_fetched = {}       # url -> bs4 object       because we need to parse result pages as we go to find more\n",
    "# this is also a record of what we do not need to add to the list fetch or parse _again_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searches don't seem to show more than 50 pages of results, so larger ranges should be split into separate searches \n",
    "interval_start = datetime.date(2024, 6, 1) # when updating an existing set, you can choose a recent date\n",
    "days_indrement = 60  \n",
    "\n",
    "while interval_start < datetime.date.today():\n",
    "    interval_end = interval_start + datetime.timedelta(days=days_indrement)\n",
    "    result_pages_to_fetch.add(\n",
    "        #'https://www.rijksoverheid.nl/documenten?type=Woo%2Dbesluit&dateRange=specific&startdatum=%s&einddatum=%s'%(\n",
    "        'https://www.rijksoverheid.nl/documenten?type=Woo%%2Dbesluit&startdatum=%s&einddatum=%s'%(\n",
    "            interval_start.strftime('%d-%m-%Y'),  # e.g. 01-01-2022\n",
    "            interval_end.strftime('%d-%m-%Y'),\n",
    "        )\n",
    "    )\n",
    "    interval_start = interval_end\n",
    "\n",
    "# We _could_ also fetch Wob (type=Wob-verzoek  instead of type=Woo-besluit) but are not _currently_ interested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the pagination pages. This should take maybe fifteen minutes per year (mostly in the backoff)\n",
    "while len(result_pages_to_fetch) > 0:   # we add numbered pagination pages as we go\n",
    "    result_page_url = result_pages_to_fetch.pop()                 # pick a page to do next\n",
    "    print('PAGE', result_page_url)\n",
    "    page_bytes = wetsuite.helpers.net.download( result_page_url ) # fetch,\n",
    "    soup = bs4.BeautifulSoup(page_bytes)                          # parse the HTML  (see bs4 documetation)\n",
    "    result_pages_fetched[result_page_url] = soup                  # cache for later use\n",
    "    for a in soup.select(\"ul.paging__numbers li a\"):              # look for links to other pages\n",
    "        other_page_url = a.get('href')\n",
    "        if 'pagina' in other_page_url  and  other_page_url not in result_pages_fetched:\n",
    "            result_pages_to_fetch.add(other_page_url)             # add them to the 'still to fetch' list as we are fetching\n",
    "            if 'pagina=50' in other_page_url:\n",
    "                warnings.warn('Arrived at page 50 for a search, chances are we are are missing some data')\n",
    "    time.sleep(2)  # be slightly nice to the server  (makes up most of the time spent)\n",
    "    # CONSIDER have progress bar -- with accordingly varying max   #len(result_pages_to_fetch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch case detail pages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each result page contains a number of cases,\n",
    "each case link is to a detail page,\n",
    "that detail page typically containing a short paragraph and links to one or more PDFs.\n",
    "\n",
    "So based on the above, we can\n",
    "- fetch and cache those detail pages\n",
    "- fetch and cache the PDFs those detail pages link to\n",
    "\n",
    "We cache the documents because they amount to gigabytes (currently ~2500 cases, times a bunch of MByte of one or more PDFs per case, is maybe 40GByte).\n",
    "That amount of data should take an hour or two to fetch from scratch, longer if you're being nice to the servers.\n",
    "\n",
    "Cacheing the _details_ page is based on the assumption that these are finished, not evolving cases; TODO: check that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from importlib import reload\n",
    "#reload(wetsuite.helpers.localdata)\n",
    "\n",
    "\n",
    "# some of these are only later than others\n",
    "# fetched\n",
    "woo_detail_pages     = wetsuite.helpers.localdata.LocalKV(  'woo-besluiten_detailpages.db', key_type=str,value_type=bytes)    # url -> page_bytes\n",
    "woo_linked_docs      = wetsuite.helpers.localdata.LocalKV(  'woo-besluiten_docs.db',        key_type=str,value_type=bytes)    # url -> content_bytes\n",
    "# collected\n",
    "woo_metadata         = wetsuite.helpers.localdata.MsgpackKV('woo-besluiten_meta.db',        key_type=str,value_type=None)     # case -> metadata_dict\n",
    "# generated\n",
    "woo_linked_docs_txt  = wetsuite.helpers.localdata.LocalKV(  'woo-besluiten_docs_txt.db',    key_type=str,value_type=str)      # url -> text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarizing metadata into a dataset\n",
    "\n",
    "def handle_detail_page(url):\n",
    "    ## Fetch that detail HTML page and parse various metadata out of that detail-page HTML.   \n",
    "    try:   \n",
    "        # cached fetch , we assume it's answered once and won't change over time -- TODO: check that's true, we could in theory adhere to HTTP cacheing rules better\n",
    "        #detail_page_bytes = wetsuite.helpers.net.download( detail_page_absurl )\n",
    "        detail_page_bytes, came_from_cache = wetsuite.helpers.localdata.cached_fetch( woo_detail_pages, url )\n",
    "    except ValueError as ve:\n",
    "        print('SKIP, error  %s  while fetching detail page  %r'%(ve, url))\n",
    "        return\n",
    "    \n",
    "    soup = bs4.BeautifulSoup( detail_page_bytes )\n",
    "    entry_metadata = {\n",
    "        'detail_page_url':       url,\n",
    "        'title':                 soup.find('h1').text.strip(),\n",
    "        'response_document_url': None,                                    # the decision to the request, with reasoning\n",
    "        'attachments':           [],   # - zero or more documents (each downloading on their own HTML page). See notes above on the variation of what is in here\n",
    "        'dates':                 [],   \n",
    "        'onderwerpen':           [],   # subject       (div.linkBlock)\n",
    "        'responsible':           None, # who responded (div.belongsto).   not a list, only ever one\n",
    "    }\n",
    "\n",
    "    # fish out the dates,\n",
    "    for mt in soup.select('p.article-meta, p.meta'):\n",
    "        mtt = mt.text.strip()\n",
    "        if 'pagina' in mtt:\n",
    "            continue\n",
    "        if '|' in mtt:\n",
    "            mtd = mtt.split('|',1)[1].strip()\n",
    "            try:\n",
    "                entry_metadata['dates'].append( dateutil.parser.parse(mtd).strftime('%Y-%m-%d') ) # reformat ISO style \n",
    "            except:\n",
    "                print(\"WARN: didn't understand %r as date\"%mtd)\n",
    "    # subject and answerer(?),\n",
    "    for lb in soup.select('div.linkBlock a'):\n",
    "        entry_metadata['onderwerpen'].append( lb.text.strip() )\n",
    "    for bt in soup.select('div.belongsTo a'):\n",
    "        entry_metadata['responsible'] = bt.text.strip()\n",
    "\n",
    "    ## Many pages seem to follow this format\n",
    "    #  main document, often the decision \n",
    "    alist = soup.select('div.article.content div.intro a')\n",
    "    if len(alist) == 0:\n",
    "        pass\n",
    "        #print( \"no div.intro a\" )\n",
    "    else: # assume it's len 1, CONSIDER: check\n",
    "        besluit_a = alist[0]\n",
    "        besluit_absurl = urllib.parse.urljoin( url, besluit_a.get('href') ) # urljoin in case they're relative (I think they're not)\n",
    "        entry_metadata['response_document_url'] = besluit_absurl\n",
    "    #  additional attachment documents (note: links to the respective download pages, not to the documents themselves)\n",
    "    for attachment_li in soup.select('div.results ul.common li'):\n",
    "        attachment_a = attachment_li.find('a')\n",
    "        entry_metadata['attachments'].append( (urllib.parse.urljoin( url, attachment_a.get('href') ),\n",
    "                                                attachment_a.find('h3').text.strip()) )\n",
    "\n",
    "    ## ...unless the page is different.\n",
    "    #   neither of the above blocks will have collected nothing (so no real need to make it conditional)\n",
    "    #   and this probably will instad\n",
    "    for adlc in soup.select('div.download a.download-chunk'):\n",
    "        adlc_absurl = urllib.parse.urljoin( url, adlc.get('href') ) \n",
    "        name = adlc.find('h2').text.strip()\n",
    "\n",
    "        if name.startswith(\"Download '\"): # clean up link text like  \"Download 'title'\"  to  \"title\"\n",
    "            name = name[10:].rstrip(\"'\")\n",
    "\n",
    "        #if name.lower().startswith('bijlage') or 'inventaris' in name.lower():\n",
    "        #    pass\n",
    "\n",
    "        # whitelist style to get complaints rather than silently ignoring\n",
    "        if re.search('^(besluit|([12345]e )?deelbesluit|woo-besluit|aanvullend besluit|herstelbesluit|beslissing)', name.lower()) is not None:\n",
    "            if entry_metadata.get('response_document_url', None) is None:\n",
    "                entry_metadata['response_document_url'] = adlc_absurl\n",
    "            else: # assume it's an attachment?\n",
    "                entry_metadata['attachments'].append( (adlc_absurl, name) )\n",
    "\n",
    "        else: # assume it's an attachment?\n",
    "            entry_metadata['attachments'].append( (adlc_absurl, name) )\n",
    "\n",
    "    ## complain  if we still didn't find any document links\n",
    "    if entry_metadata['response_document_url'] is None and len(entry_metadata['attachments']) == 0:\n",
    "        if '[ingetrokken]' in entry_metadata['title'].lower():\n",
    "            print('NOTE: no document, seems fine because [ingetrokken], on %r'%url)\n",
    "            # actually there may still be a link if retracted, see https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/07/05/besluit-op-woo-verzoek-over-documentatie-tussen-ministerie-van-ezk-en-energiebedrijven-rwe-en-uniper\n",
    "        else:\n",
    "            print(\"WARN: no document links (%s, %s) found on %r\"%(\n",
    "                entry_metadata['response_document_url'], \n",
    "                len(entry_metadata['attachments']),\n",
    "                url,\n",
    "                )) # there are a few cases like this, that's fine\n",
    "\n",
    "    # store the entry's metadata into a store\n",
    "    woo_metadata.put( url, entry_metadata )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#woo_metadata.truncate() # when redoing that dataset, ensure there's no different data from a previous run\n",
    "\n",
    "# we work from our earlier pagination fetch, it being a _current_ list of cases\n",
    "for result_page_url, result_page_soup in wetsuite.helpers.notebook.ProgressBar( result_pages_fetched.items(), description='pages'):\n",
    "\n",
    "    # Take each parsed search/browse-result page, find the HTML links to the detail pages\n",
    "    #   Note that this is scraping, so much of this is contingent on the generated HTML not doing a structural change.\n",
    "    #print( 'DOING PAGE', result_page_url )\n",
    "\n",
    "    for li in result_page_soup.select('main ol.results li.results__item'):\n",
    "        # each result item on that page is mostly a short summary, \n",
    "        #   and a link to a detail page at another URL, which duplicates most information so we only focus on the detail page\n",
    "\n",
    "        a = li.select('a.publication')[0] # assumes there is just one a in the item / li\n",
    "        \n",
    "        url = urllib.parse.urljoin( result_page_url, a.get('href') ) # relative to the page, so resolve it relative to the page URL we're on\n",
    "        print( url )\n",
    "\n",
    "\n",
    "        handle_detail_page(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('https://www.rijksoverheid.nl/documenten/woo-besluiten/2022/08/22/besluit-wob-verzoek-wijziging-subsidiecriteria-led-belichting',\n",
       "  {'detail_page_url': 'https://www.rijksoverheid.nl/documenten/woo-besluiten/2022/08/22/besluit-wob-verzoek-wijziging-subsidiecriteria-led-belichting',\n",
       "   'title': 'Besluit Wob-verzoek wijziging subsidiecriteria LED-belichting',\n",
       "   'response_document_url': 'https://open.overheid.nl/repository/ronl-3c02f5f55d4cdbb36a83a8c4f64b51e034871689/1/pdf/woo-totstandkoming-wijzigingsregeling-lnv-besluit.pdf',\n",
       "   'attachments': [],\n",
       "   'dates': ['2022-08-22'],\n",
       "   'onderwerpen': ['Landbouw en tuinbouw'],\n",
       "   'responsible': 'Ministerie van Landbouw, Natuur en Voedselkwaliteit'}),\n",
       " ('https://www.rijksoverheid.nl/documenten/woo-besluiten/2024/10/25/besluit-op-woo-verzoek-over-aanslag-motorrijtuigenbelasting',\n",
       "  {'detail_page_url': 'https://www.rijksoverheid.nl/documenten/woo-besluiten/2024/10/25/besluit-op-woo-verzoek-over-aanslag-motorrijtuigenbelasting',\n",
       "   'title': 'Besluit op Woo-verzoek over aanslag motorrijtuigenbelasting',\n",
       "   'response_document_url': 'https://open.overheid.nl/documenten/10a4ba5b-daba-4bac-8dd4-870e80e066a4/file',\n",
       "   'attachments': [],\n",
       "   'dates': ['2024-10-25'],\n",
       "   'onderwerpen': ['Belasting betalen'],\n",
       "   'responsible': 'Ministerie van FinanciÃ«n'}),\n",
       " ('https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/06/20/besluit-op-woo-verzoek-over-cites-2-b-soorten',\n",
       "  {'detail_page_url': 'https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/06/20/besluit-op-woo-verzoek-over-cites-2-b-soorten',\n",
       "   'title': 'Besluit op Woo-verzoek over CITES 2-B soorten',\n",
       "   'response_document_url': 'https://open.overheid.nl/documenten/a361527e-9843-4eac-841b-335be4b0e2dd/file',\n",
       "   'attachments': [],\n",
       "   'dates': ['2023-06-20'],\n",
       "   'onderwerpen': ['Natuur en biodiversiteit',\n",
       "    'Dierenwelzijn',\n",
       "    'Diergezondheid'],\n",
       "   'responsible': 'Ministerie van Landbouw, Natuur en Voedselkwaliteit'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how cases do we now know about?\n",
    "print( len( woo_metadata ) )\n",
    "\n",
    "# what does that information look like?\n",
    "random.sample( woo_metadata.items(), 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contents\n",
    "\n",
    "The above stored a case's metadata, let's see about the documents it contains.\n",
    "\n",
    "The response PDFs can be relatively large, and contain a lot of images of text.\n",
    "Giving you those PDFs as-is would lead to a gigabytes-large dataset, that wouldn't compress well.\n",
    "\n",
    "Interesting to some, certainly, yet let's assume for a moment that our research interest \n",
    "is just the arguments made about releasing the data.\n",
    "\n",
    "This is a simpler start, in that that should be manageable to wrangle into just text\n",
    "without too much work, in part because that decision text is likely PDFs-with-extractable text.\n",
    "(Even _if_ they are part of a larger franken-PDF, that part should be moderately easy to find)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woo_linked_docs_txt.truncate()\n",
    "\n",
    "for detail_page_url, entry_metadata in wetsuite.helpers.notebook.ProgressBar( woo_metadata.items(), description='cases' ):\n",
    "    ### Check that the detail page actually did link to a response document  (arguably can/should be done earlier)        \n",
    "    response_document_url = entry_metadata['response_document_url']\n",
    "    if response_document_url is None: # (no need here anymore?)\n",
    "        print('SKIP CASE; detail page seems to have no response doc?   %s'%entry_metadata)\n",
    "        # TODO: deal with the HTML variation for the cases that actually, there definitely is.\n",
    "        continue\n",
    "\n",
    "    # Fetch the document that we thought was the decision\n",
    "    #   note this is sometimes dozens of megabytes - there are some 300-page documents in there\n",
    "    try:\n",
    "        doc_bytes, came_from_cache = wetsuite.helpers.localdata.cached_fetch( woo_linked_docs, response_document_url, sleep_sec=5, timeout=60 )\n",
    "    except ValueError as ve:\n",
    "        print( 'SKIP CASE; response doc failed to fetch: %s for %r   on detail page %r'%(ve, response_document_url, detail_page_url ) )\n",
    "        continue\n",
    "\n",
    "    ### Check that it's a PDF\n",
    "    if not doc_bytes.startswith(b'%PDF-'):\n",
    "        print( \"SKIP CASE; not sure what kind of response document %r is, first bytes are %r\"%(\n",
    "            response_document_url, doc_bytes[:25]) )\n",
    "        continue\n",
    "    \n",
    "    # Okay, we have a document and it's a PDF.\n",
    "    # Extract page text as reported by the PDF itself   \n",
    "    #   (no OCR necessary for most of the written response - other contents are a mess in and of itself, that we luckily chose to not address yet)\n",
    "    pages_text = list( wetsuite.extras.pdf.page_text( doc_bytes ) ) # (explicit list() because generator)\n",
    "\n",
    "    # The smallest of cleanup:  try to remove the page number lines on each page  (which should only appear in the footer - not that we're testing for that)\n",
    "    pages_temp = []\n",
    "    for page_text in pages_text:\n",
    "        ptemp = page_text\n",
    "        ptemp = re.sub( r'\\n[Pp]agina(?:nummer)?\\s+[0-9]+(?:\\s+van\\s[0-9]+)?', ' ', page_text ) \n",
    "        #ptemp = re.sub( r'\\n\\s*[1-9][0-9]?}[\\s\\n]*\\Z', ' ', ptemp, flags=re.M ) # seems a little too fuzzy without also using location\n",
    "        pages_temp.append( ptemp )\n",
    "    pages_text = pages_temp\n",
    "\n",
    "    all_text = '\\n'.join( pages_text )\n",
    "\n",
    "    woo_linked_docs_txt.put(response_document_url, all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://open.overheid.nl/documenten/8ae672ca-b1f1-4886-8a5d-abb42dce7c66/file\n",
      "\"> Retouradres Postbus 20401 2500 EK Den Haag\\n \\nDirectoraat-generaal Natuur \\nen Visserij \\n5.1.2.e\\nBezoekadres \\nBezuidenhoutseweg 73 \\n2594 AC Den Haag \\n \\nPostadres \\nPostbus 20401 \\n2500 EK De\n",
      "\n",
      "https://open.overheid.nl/documenten/274212b4-9ec6-412d-97b2-481471135cef/file\n",
      "'1.\\nVan: \\n \\n5.1.2.e\\nVerzonden: \\nvrijdag 25 februari 2022 09:59 \\nAan: \\n \\n5.1.2.e\\nOnderwerp: \\nFW: Vullen Bergermeer door EBN \\nBijlagen: \\nDOMUS-22077637-v1-\\nHet_vullen_van_de_gasopslag_Berge\n",
      "\n",
      "https://open.overheid.nl/documenten/a08551b6-a9ab-4b71-a22e-4ea8572d9b07/file\n",
      "'> Retouradres Postbus 40219 8004 DE Zwolle\\n \\nRijksdienst voor \\nOndernemend Nederland \\nVergunningen en handhaving \\n \\nMandemaat 3, Assen \\nPostbus 40219 \\n8004 DE Zwolle \\nwww.rvo.nl \\nContactper\n",
      "\n",
      "https://open.overheid.nl/documenten/296b7311-b07a-4621-bf49-b46c694422f8/file\n",
      "' \\nNederlandse Voedsel- en \\nWarenautoriteit \\nMinisterie van Landbouw, \\nNatuur en Voedselkwaliteit \\n \\n \\n \\n> Retouradres Postbus 43006 3540 AA  Utrecht \\n \\nUITSLUITEND PER E-MAIL GESTUURD  \\n \\\n",
      "\n",
      "https://open.overheid.nl/documenten/0a4566ec-5474-4723-a0ac-0c6dfb938f74/file\n",
      "' \\nMinisterie van Infrastructuur \\nen Waterstaat \\n \\n \\n \\n Retouradres Postbus 20901 2500 EX Den Haag \\n-\\nBestuurskern \\nDirectie Algemeen Strategisch \\nAdvies  \\n \\nPostbus 20901  \\n2500 EX Den H\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for url, text in woo_linked_docs_txt.random_sample(5):\n",
    "    print( f'{url}\\n{repr(text)[:200]}\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are headed out to be datasets, so set some supporting store metadata\n",
    "\n",
    "#### Meta\n",
    "woo_metadata._put_meta('valtype','msgpack')\n",
    "woo_metadata._put_meta('description_short','A dataset focusing on the reaction documents to Woo requests; this gives basic metadata. ')\n",
    "woo_metadata._put_meta('description','''\n",
    "This dataset tries to focus on the reactions to Woo requests.\n",
    "\n",
    "its .data is a a map \n",
    "- from an unique case identifier (not necessarily meaningful, currently happens to be the case's URL on the website, looks like 'https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/08/31/besluit-op-woo-verzoek-geen-documenten-over-gewasbeschermingsregistratie')\n",
    "- to a dict like:\n",
    "    {'detail_page_url':       'https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/08/31/besluit-op-woo-verzoek-geen-documenten-over-gewasbeschermingsregistratie',\n",
    "     'title':                 'Besluit op Woo-verzoek geen documenten over gewasbeschermingsregistratie',\n",
    "     'response_document_url': 'https://open.overheid.nl/documenten/ef04ac22-1eb9-4b5c-a439-39fb3b636fdf/file',\n",
    "     'attachments':           [],\n",
    "     'dates':                 ['2023-08-31'],\n",
    "     'onderwerpen':           ['Bestrijdingsmiddelen'],\n",
    "     'responsible':           'Ministerie van Landbouw, Natuur en Voedselkwaliteit'\n",
    "    }\n",
    "\n",
    "You are proably also interested in wetsuite.datasets.load()-ing the related dataset that has the text for the response documents (fetched by response_document_url)\n",
    "\n",
    "Both could use some refinement.   TODO: clean up both the woo metadata and woo text datasets                       \n",
    "''')\n",
    "\n",
    "\n",
    "#### Text\n",
    "woo_linked_docs_txt._put_meta('description_short','A dataset focusing on the reaction documents to Woo requests; this is a plain text variant of the PDF text. ')\n",
    "woo_linked_docs_txt._put_meta('description','''\n",
    "This dataset tries to focus on the reactions to Woo requests.\n",
    "                                                     \n",
    "You will probably want the Woo metadata dataset first.\n",
    "It will have `response_document_url` keys linking to a PDF document.\n",
    "\n",
    "This is a map from that URL to the a plaintext string of the text that PDF contains.\n",
    "                              \n",
    "Notes:\n",
    "- if we figure it was something other than a decision, it may not be present\n",
    "- the extraction is currenty 'the text the PDF itself reports'\n",
    "    \n",
    "Both could use some refinement.   TODO: clean up both the woo metadata and woo text datasets\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " number of cases:                 4697\n",
      " decision doc texts they link to: 3884\n"
     ]
    }
   ],
   "source": [
    "# glancing in spection: TODO: fix those names, are clearly wrong :)\n",
    "print( ' number of cases:                ', len( woo_metadata        ) )\n",
    "print( ' decision doc texts they link to:', len( woo_linked_docs_txt ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5984256, 266903552)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(woo_metadata.bytesize(), woo_linked_docs_txt.bytesize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "woo_metadata.close()\n",
    "woo_linked_docs_txt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
