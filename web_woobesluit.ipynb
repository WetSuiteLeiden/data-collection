{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/WetSuiteLeiden/data-collection/blob/master/web_woobesluit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of this notebook\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch documents from [rijksoverheid.nl/documenten](https://www.rijksoverheid.nl/documenten?type=Woo-besluit),\n",
    "focusing first and foremost on [Woo](https://nl.wikipedia.org/wiki/Wet_openbaarheid_van_bestuur#Wet_open_overheid) requests.\n",
    "\n",
    "We abstract out the \"scrape result items from this site's pagination\" into a module, \n",
    "but each type of document (Woo is just one) deserves its own handling, \n",
    "both in terms of what you want to do with the contents, and that you probably want to cache the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That website, asked for any type of document as an example search, broadly gives:\n",
    "* Browse/search result pages \n",
    "  - looks the same for most - a title-and-link, and a summary\n",
    "  - for Woo looks like https://www.rijksoverheid.nl/documenten?type=Woo%2Dbesluit&pagina=45\n",
    "\n",
    "* A case's detail page \n",
    "  - the contents of these varies a lot with requested document type, and a little more within some docuument types. For some, it contains everything. For others, just a link to a download. For some, a mix.\n",
    "  - for Woo looks like looks like: https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/05/22/besluit-op-woo-verzoek-over-alle-eendenhouderijen-nederland - that is, primarily a link to a 'attached' PDF\n",
    "\n",
    "* additionally, various types of detail page downloads, embeds, and/or links elsewhere\n",
    "  - for Woo this is typically a PDF, e.g. https://open.overheid.nl/documenten/76f1a787-3f8a-452c-bf58-5911e1a89bcd/file \n",
    "  - e.g. https://www.rijksoverheid.nl/documenten/mediateksten/2025/01/10/letterlijke-tekst-persconferentie-na-ministerraad-10-januari-2025 has a link to youtube\n",
    "  - e.g. https://www.rijksoverheid.nl/documenten/geluidsfragmenten/2024/10/15/gesproken-versie-van-hoe-voorkom-ik-koolmonoxidevergiftiging-in-mijn-huis has an embed and download to a MP3 file hosted elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime, random, warnings\n",
    "import urllib.parse, re, pprint\n",
    "\n",
    "import bs4\n",
    "import dateutil.parser\n",
    "\n",
    "import wetsuite.datacollect.rijksoverheid_nl_documenten\n",
    "import wetsuite.helpers.net\n",
    "import wetsuite.helpers.strings\n",
    "import wetsuite.helpers.notebook\n",
    "import wetsuite.helpers.localdata\n",
    "import wetsuite.helpers.patterns\n",
    "import wetsuite.extras.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we get to Woo, let's start with an easier example \n",
    "\n",
    "The case of Woo has a few hairy details, so if you are interested in fetching other things from [rijksoverheid.nl/documenten](https://www.rijksoverheid.nl/documenten?type=Woo-besluit), keep reading this section. If only interested in Woo, skip straight to that part.\n",
    "\n",
    "Let's start with the `Mediatekst` document type, which seems to be press conferences, press releases, and such.\n",
    "\n",
    "As mentioned, while the pagination is handled for you, but have to figure out what to do with the detail page it links to.\n",
    "\n",
    "For the mentioned press pages within this set, those pages seem to be just a single HTML page with all the text,\n",
    "so just downloading them and packing them into a small dataset should already be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, almost easy.  Let's also make this a dataset we will provide. That means we have to create the store in a slightly specific way:\n",
    "mediatekst_html_store = wetsuite.helpers.localdata.LocalKV('mediatekst-html.db',        key_type=str,value_type=bytes) # url to htmlbytes\n",
    "mediatekst_html_store._put_meta('description_short','A dataset with press conderences and releases from www.rijksoverheid.nl/documenten ')\n",
    "mediatekst_html_store._put_meta('description','''\n",
    "This dataset stores all documents of type 'Mediatekst' from https://www.rijksoverheid.nl/documenten\n",
    "which seems to be primarily conferences and press releases.\n",
    "\n",
    "Each value is the HTML file as a bytestring, unprocessed from how we found it.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_mediatekst_detail_page(soup, url):\n",
    "    ''' scrape_pagination wants to call something for each detail page. \n",
    "        In this case that page _is_ the content, so we only care to fetch it.\n",
    "    '''\n",
    "    print('ITEM',url)\n",
    "    _, came_from_cache = wetsuite.helpers.localdata.cached_fetch(mediatekst_html_store, url)\n",
    "    if not came_from_cache:\n",
    "        time.sleep(2)\n",
    "\n",
    "\n",
    "# currently under 300 items, should take maybe 20 minutes\n",
    "wetsuite.datacollect.rijksoverheid_nl_documenten.scrape_pagination(\n",
    "    doctype='Mediatekst',\n",
    "    from_date=datetime.date(year=2010, month=1, day=1), # we know this store currently contains nothing before then \n",
    "    to_date  =datetime.date.today(), \n",
    "    detail_page_callback=handle_mediatekst_detail_page, \n",
    "    debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'size_bytes': 15044608,\n",
       " 'size_readable': '14.3MiB',\n",
       " 'num_items': 276,\n",
       " 'avgsize_bytes': 54509,\n",
       " 'avgsize_readable': '53KiB'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mediatekst_html_store.summary(get_num_items=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving on to Woo\n",
    "\n",
    "We can fetch responses to Woo reqests when we [rijksoverheid.nl/documenten](https://www.rijksoverheid.nl/documenten?type=Woo-besluit) if we filter for `type=Woo-besluit`.\n",
    "\n",
    "It should be clearly pointed out that, due to the way Woo requests are handled, **there is no singular complete source of Woo requests** (yet?).\n",
    "\n",
    "...that the government provides. The third-party [woogle](https://woogle.wooverheid.nl/search?q=*&page=1&type=2i) is probably your best bet for... well, at least [_more_](https://woogle.wooverheid.nl/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on detail pages\n",
    "\n",
    "This is also one of the more complex examples of fetching things from this site,\n",
    "in that **there is variation both in how detail pages work, and how the fetched documents are structured**.\n",
    "\n",
    "Consider:\n",
    "* https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/06/20/besluit-op-woo-verzoek-over-cites-2-b-soorten\n",
    "  - is an overall rejection so only has a decision document\n",
    "\n",
    "* https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/06/22/besluit-op-woo-verzoek-over-de-vergaderingen-van-de-kerngroep-bloembollen\n",
    "  - has a single document that is decision + inventory + contents\n",
    "\n",
    "* https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/06/14/besluit---woo-verzoek-ongebruikelijke-transacties-estland-letland-en-litouwen\n",
    "  - has a separate decision document, and document that is the contents of the response, here a bunch of tables\n",
    "\n",
    "* https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/06/15/besluit-op-woo-verzoek-over-correspondentie-naar-aanleiding-van-fouten-in-de-lijst-met-top-100-ammoniakuitstoters\n",
    "  - has a separate decision, and the bijlage/documents is actually a link to _another_ detail page first -- which breaks our otherwise-mostly-correct assumption that every page is a case\n",
    "\n",
    "* https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/06/21/besluit-op-woo-verzoek-over-vertraging-van-de-bouw-van-stikstofinstallatie-zuidbroek-ii\n",
    "  - is a separate decision. It also separates inventaris and seven separate content documents, also each via their own detail page.\n",
    "\n",
    "* https://open.overheid.nl/documenten/a645fe8c-6c58-4e75-b355-aa3a97023eb8/file\n",
    "  - has a document that is the decision + inventory, and points out the real data is large files should be requested for via mail\n",
    "\n",
    "* https://open.overheid.nl/documenten/107e45ab-6533-4bda-bfc2-816ce107906e/file\n",
    "  - is images of a besluit document. The PDF contains no text layer / OCR.\n",
    "\n",
    "* https://open.overheid.nl/documenten/ronl-439dfebe8cffecb9a385633cb757ced59de469ee/pdf\n",
    "  - has one page of OCR-less image-of-text, then goes on to actual text in the middle of a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch case-detail pages\n",
    "\n",
    "As such, creating a dataset with more consistency than provided to us in the first place\n",
    "will take more work, and some creativity.\n",
    "\n",
    "For now, let's only care about the motivation behind the decision, and not the document(s) that the request/decision implies,\n",
    "if only to keep our task reasonable for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get a little more practical:\n",
    "- each result page contains a number of cases, each case link is to a detail page; this is handled for us.\n",
    "- that detail page typically containing a short paragraph and links to one or more PDFs.\n",
    "  - let's always cache the linked document - saves time and resources (~2500 cases, times a bunch of MByte per document, times a couple per case, is maybe 40GByte and hours to days of fetching)\n",
    "  - let's assume these are always _finished_ cases, not evolving ones, so that the detail pages themselves can also be cached; TODO: check that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch and cache data as it is provided - both the detail pages and the documents that are linked to\n",
    "woo_detail_pages     = wetsuite.helpers.localdata.LocalKV(  'woo-besluiten_detailpages.db', key_type=str,value_type=bytes)    # url -> page_bytes\n",
    "woo_linked_docs      = wetsuite.helpers.localdata.LocalKV(  'woo-besluiten_docs.db',        key_type=str,value_type=bytes)    # url -> content_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are headed out to be datasets, so create the stores that will be those datasets.\n",
    "\n",
    "## Metadata\n",
    "# It was a choice to collect the metadata and text into separate datasets.\n",
    "# As things are, we do this during detail-page handling\n",
    "woo_metadata         = wetsuite.helpers.localdata.MsgpackKV('woo-besluiten_meta.db',        key_type=str,value_type=None)     # case -> metadata_dict\n",
    "#woo_metadata.truncate() # when redoing that dataset, ensure there's no data from a previous run\n",
    "woo_metadata._put_meta('description_short','A dataset focusing on the reaction documents to Woo requests; this gives basic metadata. ')\n",
    "woo_metadata._put_meta('description','''\n",
    "This dataset tries to focus on the reactions to Woo requests.\n",
    "\n",
    "its .data is a a map \n",
    "- from an unique case identifier (not necessarily meaningful, currently happens to be the case's URL on the website, looks like 'https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/08/31/besluit-op-woo-verzoek-geen-documenten-over-gewasbeschermingsregistratie')\n",
    "- to a dict like:\n",
    "    {'detail_page_url':       'https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/08/31/besluit-op-woo-verzoek-geen-documenten-over-gewasbeschermingsregistratie',\n",
    "     'title':                 'Besluit op Woo-verzoek geen documenten over gewasbeschermingsregistratie',\n",
    "     'response_document_url': 'https://open.overheid.nl/documenten/ef04ac22-1eb9-4b5c-a439-39fb3b636fdf/file',\n",
    "     'attachments':           [],\n",
    "     'dates':                 ['2023-08-31'],\n",
    "     'onderwerpen':           ['Bestrijdingsmiddelen'],\n",
    "     'responsible':           'Ministerie van Landbouw, Natuur en Voedselkwaliteit'\n",
    "    }\n",
    "\n",
    "You are proably also interested in wetsuite.datasets.load()-ing the related dataset that has the text for the response documents (fetched by response_document_url)\n",
    "\n",
    "Both could use some refinement.   TODO: clean up both the woo metadata and woo text datasets                       \n",
    "''')\n",
    "\n",
    "\n",
    "## Text\n",
    "# somewhere later, we generate text from the PDFs via OCR, which is slow so let's also cache that \n",
    "woo_linked_docs_txt  = wetsuite.helpers.localdata.LocalKV(  'woo-besluiten_docs_txt.db',    key_type=str,value_type=str)      # url -> text\n",
    "woo_linked_docs_txt._put_meta('description_short','A dataset focusing on the reaction documents to Woo requests; this is a plain text variant of the PDF text. ')\n",
    "woo_linked_docs_txt._put_meta('description','''\n",
    "This dataset tries to focus on the reactions to Woo requests.\n",
    "                                                     \n",
    "You will probably want the Woo metadata dataset first.\n",
    "It will have `response_document_url` keys linking to a PDF document.\n",
    "\n",
    "This is a map from that URL to the a plaintext string of the text that PDF contains.\n",
    "                              \n",
    "Notes:\n",
    "- if we figure it was something other than a decision, it may not be present\n",
    "- the extraction is currenty 'the text the PDF itself reports'\n",
    "    \n",
    "Both could use some refinement.   TODO: clean up both the woo metadata and woo text datasets\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarizing metadata into a dataset\n",
    "\n",
    "def handle_woobesluit_detail_page(soup, url):\n",
    "    ''' This function only fetches the detail page and fill metadata.\n",
    "        We could also fetch and parse PDFs at the same time -- but we can also do that later.\n",
    "    '''\n",
    "    print('ITEM',url)\n",
    "\n",
    "    ### Fetch that detail HTML page and parse various metadata out of that detail-page HTML.   \n",
    "    try:   \n",
    "        # cached fetch , we assume it's answered once and won't change over time -- TODO: check that's true, we could in theory adhere to HTTP cacheing rules better\n",
    "        #detail_page_bytes = wetsuite.helpers.net.download( detail_page_absurl )\n",
    "        detail_page_bytes, came_from_cache = wetsuite.helpers.localdata.cached_fetch( woo_detail_pages, url )\n",
    "    except ValueError as ve:\n",
    "        print('SKIP, error  %s  while fetching detail page  %r'%(ve, url))\n",
    "        return\n",
    "    \n",
    "    soup = bs4.BeautifulSoup( detail_page_bytes )\n",
    "    entry_metadata = {\n",
    "        'detail_page_url':       url,\n",
    "        'title':                 soup.find('h1').text.strip(),\n",
    "        'response_document_url': None,                                    # the decision to the request, with reasoning\n",
    "        'attachments':           [],   # - zero or more documents (each downloading on their own HTML page). See notes above on the variation of what is in here\n",
    "        'dates':                 [],   \n",
    "        'onderwerpen':           [],   # subject       (div.linkBlock)\n",
    "        'responsible':           None, # who responded (div.belongsto).   not a list, only ever one\n",
    "    }\n",
    "\n",
    "    # fish out the dates,\n",
    "    for mt in soup.select('p.article-meta, p.meta'):\n",
    "        mtt = mt.text.strip()\n",
    "        if 'pagina' in mtt:\n",
    "            continue\n",
    "        if '|' in mtt:\n",
    "            mtd = mtt.split('|',1)[1].strip()\n",
    "            try:\n",
    "                entry_metadata['dates'].append( dateutil.parser.parse(mtd).strftime('%Y-%m-%d') ) # reformat ISO style \n",
    "            except:\n",
    "                print(\"WARN: didn't understand %r as date\"%mtd)\n",
    "    # subject and answerer(?),\n",
    "    for lb in soup.select('div.linkBlock a'):\n",
    "        entry_metadata['onderwerpen'].append( lb.text.strip() )\n",
    "    for bt in soup.select('div.belongsTo a'):\n",
    "        entry_metadata['responsible'] = bt.text.strip()\n",
    "\n",
    "    ## Many pages seem to follow this format\n",
    "    #  main document, often the decision \n",
    "    alist = soup.select('div.article.content div.intro a')\n",
    "    if len(alist) == 0:\n",
    "        pass\n",
    "        #print( \"no div.intro a\" )\n",
    "    else: # assume it's len 1, CONSIDER: check\n",
    "        besluit_a = alist[0]\n",
    "        besluit_absurl = urllib.parse.urljoin( url, besluit_a.get('href') ) # urljoin in case they're relative (I think they're not)\n",
    "        entry_metadata['response_document_url'] = besluit_absurl\n",
    "    #  additional attachment documents (note: links to the respective download pages, not to the documents themselves)\n",
    "    for attachment_li in soup.select('div.results ul.common li'):\n",
    "        attachment_a = attachment_li.find('a')\n",
    "        entry_metadata['attachments'].append( (urllib.parse.urljoin( url, attachment_a.get('href') ),\n",
    "                                                attachment_a.find('h3').text.strip()) )\n",
    "    ## ...unless the page is different.\n",
    "    #   neither of the above blocks will have collected nothing (so no real need to make it conditional)\n",
    "    #   and this probably will instad\n",
    "    for adlc in soup.select('div.download a.download-chunk'):\n",
    "        adlc_absurl = urllib.parse.urljoin( url, adlc.get('href') ) \n",
    "        name = adlc.find('h2').text.strip()\n",
    "\n",
    "        if name.startswith(\"Download '\"): # clean up link text like  \"Download 'title'\"  to  \"title\"\n",
    "            name = name[10:].rstrip(\"'\")\n",
    "\n",
    "        #if name.lower().startswith('bijlage') or 'inventaris' in name.lower():\n",
    "        #    pass\n",
    "\n",
    "        # whitelist style to get complaints rather than silently ignoring\n",
    "        if re.search('^(besluit|([12345]e )?deelbesluit|woo-besluit|aanvullend besluit|herstelbesluit|beslissing)', name.lower()) is not None:\n",
    "            if entry_metadata.get('response_document_url', None) is None:\n",
    "                entry_metadata['response_document_url'] = adlc_absurl\n",
    "            else: # assume it's an attachment?\n",
    "                entry_metadata['attachments'].append( (adlc_absurl, name) )\n",
    "\n",
    "        else: # assume it's an attachment?\n",
    "            entry_metadata['attachments'].append( (adlc_absurl, name) )\n",
    "\n",
    "    ## complain  if we still didn't find any document links\n",
    "    if entry_metadata['response_document_url'] is None and len(entry_metadata['attachments']) == 0:\n",
    "        if '[ingetrokken]' in entry_metadata['title'].lower():\n",
    "            print('NOTE: no document, seems fine because [ingetrokken], on %r'%url)\n",
    "            # actually there may still be a link if retracted, see https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/07/05/besluit-op-woo-verzoek-over-documentatie-tussen-ministerie-van-ezk-en-energiebedrijven-rwe-en-uniper\n",
    "        else:\n",
    "            print(\"WARN: no document links (%s, %s) found on %r\"%(\n",
    "                entry_metadata['response_document_url'], \n",
    "                len(entry_metadata['attachments']),\n",
    "                url,\n",
    "                )) # there are just a few cases like this, that's probably fine\n",
    "\n",
    "    # store the entry's metadata into a store\n",
    "    woo_metadata.put( url, entry_metadata )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently ~5000 items, might take a few hours\n",
    "wetsuite.datacollect.rijksoverheid_nl_documenten.scrape_pagination(\n",
    "    doctype='Woo-besluit',\n",
    "    from_date=datetime.date(year=2019, month=1, day=1), # we know this store currently contains nothing before then\n",
    "    to_date  =datetime.date.today(), \n",
    "    detail_page_callback=handle_woobesluit_detail_page, \n",
    "    debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'size_bytes': 6496256,\n",
       " 'size_readable': '6.2MiB',\n",
       " 'num_items': 5116,\n",
       " 'avgsize_bytes': 1270,\n",
       " 'avgsize_readable': '1.2KiB'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woo_metadata.summary(get_num_items=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/08/17/besluit-op-woo-verzoek-over-aanvragen-cites-in-of-uitvoervergunningen-voor-marokkaanse-stekelige-staarthagedis',\n",
       "  {'detail_page_url': 'https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/08/17/besluit-op-woo-verzoek-over-aanvragen-cites-in-of-uitvoervergunningen-voor-marokkaanse-stekelige-staarthagedis',\n",
       "   'title': 'Besluit op Woo-verzoek over aanvragen CITES in- of uitvoervergunningen voor Marokkaanse stekelige staarthagedis',\n",
       "   'response_document_url': 'https://open.overheid.nl/documenten/2131fdec-390c-419c-acfe-c463adf7e990/file',\n",
       "   'attachments': [],\n",
       "   'dates': ['2023-08-17'],\n",
       "   'onderwerpen': ['Herziening Woo-besluit over aanvragen CITES in- of uitvoervergunningen Marokkaanse stekelige staarthagedis',\n",
       "    'Natuur en biodiversiteit'],\n",
       "   'responsible': 'Ministerie van Landbouw, Natuur en Voedselkwaliteit'}),\n",
       " ('https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/09/04/besluit-op-woo-verzoek-over-regeling-schrijnende-gevallen-binnen-leefbaarheidsfonds-eindhoven-airport',\n",
       "  {'detail_page_url': 'https://www.rijksoverheid.nl/documenten/woo-besluiten/2023/09/04/besluit-op-woo-verzoek-over-regeling-schrijnende-gevallen-binnen-leefbaarheidsfonds-eindhoven-airport',\n",
       "   'title': 'Besluit op Woo-verzoek over regeling schrijnende gevallen binnen Leefbaarheidsfonds Eindhoven Airport',\n",
       "   'response_document_url': 'https://open.overheid.nl/documenten/00601847-670a-4f95-993f-05d9f71c0aca/file',\n",
       "   'attachments': [['https://www.rijksoverheid.nl/documenten/publicaties/2023/09/04/lijst-van-documenten-bij-woo-besluit-over-regeling-schrijnende-gevallen-binnen-leefbaarheidsfonds-eindhoven-airport',\n",
       "     'Lijst van documenten bij Woo-besluit over regeling schrijnende gevallen binnen Leefbaarheidsfonds Eindhoven Airport'],\n",
       "    ['https://www.rijksoverheid.nl/documenten/publicaties/2023/09/04/documenten-bij-woo-besluit-over-regeling-schrijnende-gevallen-binnen-leefbaarheidsfonds-eindhoven-airport',\n",
       "     'Documenten bij Woo-besluit over regeling schrijnende gevallen binnen Leefbaarheidsfonds Eindhoven Airport']],\n",
       "   'dates': ['2023-04-09', '2023-04-09', '2023-04-09'],\n",
       "   'onderwerpen': ['Luchtvaart'],\n",
       "   'responsible': 'Ministerie van Infrastructuur en Waterstaat'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what does that information look like at this stage?\n",
    "random.sample( woo_metadata.items(), 2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documents and their text contents\n",
    "\n",
    "The above stored a case's metadata, let's see about the documents it contains.\n",
    "\n",
    "The response PDFs can be relatively large, and contain a lot of images of text.\n",
    "Giving you those PDFs as-is would lead to a gigabytes-large dataset, that wouldn't compress well.\n",
    "\n",
    "Interesting to some, certainly, yet let's assume for a moment that our research interest \n",
    "is just the arguments made about releasing the data.\n",
    "\n",
    "This is a simpler start, in that that should be manageable to wrangle into plain text.\n",
    "This should be relatively little work because, unlike the requested data, the decision text is likely in PDFs with extractable text.\n",
    "(Even _if_ they are part of a larger franken-PDF, that part should be moderately easy to find)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#woo_linked_docs_txt.truncate() # wipe it whenever you change the extraction\n",
    "# this should take roughly twenty(?) minutes for 5K items\n",
    "\n",
    "# Go through those metadata items, and sift through the attachments, mostly PDFs, for what we need\n",
    "for detail_page_url, entry_metadata in wetsuite.helpers.notebook.ProgressBar( woo_metadata.items(), description='cases' ):\n",
    "    response_document_url = entry_metadata['response_document_url']\n",
    "\n",
    "    # these are some known huge (500MByte to 1GByte) PDFs that we might as well not fetch or store\n",
    "    if response_document_url in (\n",
    "        'https://open.overheid.nl/documenten/3ab07a67-c8cd-4ef5-897c-a73cf9db36ae/file',\n",
    "        'https://open.overheid.nl/documenten/c2299374-1f63-424a-aa16-28ece8f0cb43/file',\n",
    "        'https://open.overheid.nl/documenten/6998c113-a776-4442-ba21-160285727e98/file',\n",
    "        'https://open.overheid.nl/documenten/b02410c1-0286-4c72-81c0-d3081fc2954e/file',\n",
    "        'https://open.overheid.nl/documenten/172a4167-5a6b-48b6-bb63-be4b0c284ad7/file',\n",
    "    ):\n",
    "        continue\n",
    " \n",
    "    # when updating (you commented that truncate) we can do just the new documents quickly\n",
    "    # (note: would still rediscover skippable problem cases each time)\n",
    "    if response_document_url in woo_linked_docs_txt: \n",
    "        continue\n",
    "\n",
    "    ### Check that the detail page actually did link to a response document  (arguably can/should be done earlier)        \n",
    "    if response_document_url is None: # (no need here anymore?)\n",
    "        print('SKIP CASE; detail page seems to have no response doc?   %s'%entry_metadata)\n",
    "        # TODO: deal with the HTML variation for the cases that actually, there definitely is.\n",
    "        continue\n",
    "\n",
    "    # Fetch the document that we thought was the decision\n",
    "    #   note this is sometimes dozens of megabytes - there are some 300-page documents in there, and one 970MByte, 2190-page PDF\n",
    "    try:\n",
    "        doc_bytes, came_from_cache = wetsuite.helpers.localdata.cached_fetch( woo_linked_docs, response_document_url, sleep_sec=5, timeout=60 )\n",
    "    except ValueError as ve:\n",
    "        print( 'SKIP CASE; response doc failed to fetch: %s for %r   on detail page %r'%(ve, response_document_url, detail_page_url ) )\n",
    "        continue\n",
    "\n",
    "    ### Check that it's a PDF\n",
    "    if not doc_bytes.startswith(b'%PDF-'):\n",
    "        print( \"SKIP CASE; not sure what kind of response document %r is, first bytes are %r, on detail page %r\"%(\n",
    "            response_document_url, doc_bytes[:25], detail_page_url) )\n",
    "        continue\n",
    "    \n",
    "    # Okay, we have a document and it's a PDF.\n",
    "    # Extract page text as reported by the PDF itself   \n",
    "    #   (no OCR necessary for **most** of the written responses - other contents are a mess in and of itself, that we luckily chose to not address yet)\n",
    "    pages_text = list( wetsuite.extras.pdf.page_text( doc_bytes ) ) # (explicit list() because generator)\n",
    "\n",
    "    # The smallest of cleanup:  try to remove the page number lines on each page  (which should only appear in the footer - not that we're testing for that)\n",
    "    pages_temp = []\n",
    "    for page_text in pages_text:\n",
    "        ptemp = page_text\n",
    "        ptemp = re.sub( r'\\n[Pp]agina(?:nummer)?\\s+[0-9]+(?:\\s+van\\s[0-9]+)?', ' ', page_text ) \n",
    "        #ptemp = re.sub( r'\\n\\s*[1-9][0-9]?}[\\s\\n]*\\Z', ' ', ptemp, flags=re.M ) # seems a little too fuzzy without also using location\n",
    "        pages_temp.append( ptemp )\n",
    "    pages_text = pages_temp\n",
    "\n",
    "    all_text = '\\n'.join( pages_text )\n",
    "\n",
    "    woo_linked_docs_txt.put(response_document_url, all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://open.overheid.nl/documenten/ec4703fb-26cb-462e-be65-107fd9d0049f/file\n",
      "'> Retouradres Postbus 90801 2509 LV Den Haag\\nDirectie Wetgeving,\\nBestuurlijke en Juridische\\nAangelegenheden\\nPostbus 90801\\n2509 LV Den Haag\\nParnassusplein 5\\nT 070 333 44 44\\nwww.rijksoverheid.n\n",
      "\n",
      "https://open.overheid.nl/documenten/f6bc1a3d-3df4-4f83-a18c-5ab81b93ef0a/file\n",
      "'\\n \\n \\nDirectie Openbaarmaking \\n \\nDatum \\n11 november 2024 \\n \\nOnze referentie \\n \\nBesluit \\nDe gevraagde documenten zijn reeds (gedeeltelijk) openbaar. Op reeds openbare \\ninformatie is de Woo \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for url, text in woo_linked_docs_txt.random_sample(2):\n",
    "    print( url )\n",
    "    print( repr(text)[:200] )\n",
    "    print( )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'size_bytes': 6496256,\n",
       " 'size_readable': '6.2MiB',\n",
       " 'num_items': 5116,\n",
       " 'avgsize_bytes': 1270,\n",
       " 'avgsize_readable': '1.2KiB'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count is roughly the number of cases:\n",
    "woo_metadata.summary(get_num_items=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'size_bytes': 309526528,\n",
       " 'size_readable': '295MiB',\n",
       " 'num_items': 4899,\n",
       " 'avgsize_bytes': 63182,\n",
       " 'avgsize_readable': '62KiB'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count is roughly the decision doc texts they link to - that we got:\n",
    "woo_linked_docs_txt.summary(get_num_items=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
